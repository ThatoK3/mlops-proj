version: "3.8"

services:
  # ================= MySQL =================
  mysql:
    image: quay.io/debezium/example-mysql:1.9
    ports:
      - "3306:3306"
    environment:
      - MYSQL_ROOT_PASSWORD=${MYSQL_ROOT_PASSWORD}
      - MYSQL_USER=${MYSQL_USER}
      - MYSQL_PASSWORD=${MYSQL_PASSWORD}
      - MYSQL_DATABASE=${MYSQL_DATABASE}
    networks:
      - app-network
    volumes:
      - ./init:/docker-entrypoint-initdb.d
    healthcheck:
      test: ["CMD", "mysqladmin", "ping", "-h", "localhost", "-u", "root", "-p${MYSQL_ROOT_PASSWORD}"]
      interval: 10s
      timeout: 5s
      retries: 10
      start_period: 30s
    restart: unless-stopped

  # ================= MS SQL Server =================
  mssql:
    image: mcr.microsoft.com/mssql/server:2019-latest
    container_name: mssql-server
    environment:
      - ACCEPT_EULA=Y
      - SA_PASSWORD=${MSSQL_SA_PASSWORD:-YourStrong!Pass123}
      - MSSQL_PID=Developer
      - MSSQL_DATABASE=stroke_predictions_sink
    ports:
      - "1433:1433"
    networks:
      - app-network
    volumes:
      - mssql_data:/var/opt/mssql
      - ./init/mssql-init.sql:/docker-entrypoint-initdb.d/init.sql
    healthcheck:
      test: /opt/mssql-tools/bin/sqlcmd -S localhost -U sa -P "$$SA_PASSWORD" -Q "SELECT 1" || exit 1
      interval: 10s
      timeout: 5s
      retries: 10
      start_period: 30s
    restart: unless-stopped

  # ================= Kafka + Debezium + Schema Registry =================
  zookeeper:
    image: confluentinc/cp-zookeeper:5.5.3
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
    networks:
      - app-network
    restart: unless-stopped

  kafka:
    image: confluentinc/cp-enterprise-kafka:5.5.3
    environment:
      KAFKA_ZOOKEEPER_CONNECT: "zookeeper:2181"
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:9092
      KAFKA_BROKER_ID: 1
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_JMX_PORT: 9991
    ports:
      - "9092:9092"
    networks:
      - app-network
    depends_on:
      - zookeeper
    restart: unless-stopped

  connect:
    image: quay.io/debezium/connect:1.9
    ports:
      - "8083:8083"
      - "9400:9400"
    environment:
      - BOOTSTRAP_SERVERS=kafka:9092
      - GROUP_ID=1
      - CONFIG_STORAGE_TOPIC=my_connect_configs
      - OFFSET_STORAGE_TOPIC=my_connect_offsets
      - STATUS_STORAGE_TOPIC=my_connect_statuses
      - KAFKA_OPTS=-javaagent:/kafka/connect/jmx_exporter/jmx_prometheus_javaagent-0.19.0.jar=9400:/kafka/connect/jmx_exporter/config.yaml
    volumes: 
      - ${PWD}/kconnect-jdbc-sink-jars:/kafka/connect/kconnect-jdbc-sink-jars
      - ${PWD}/kconnect-s3-sink-jars:/kafka/connect/kconnect-s3-sink-jars
      - ${PWD}/confluentic-connect-transforms:/kafka/connect/confluentic-connect-transforms
      - ${PWD}/jmx_exporter:/kafka/connect/jmx_exporter
    networks:
      - app-network
    depends_on:
      - kafka
      - mysql
    restart: unless-stopped

  schema-registry:
    image: confluentinc/cp-schema-registry:5.5.3
    environment:
      - SCHEMA_REGISTRY_KAFKASTORE_CONNECTION_URL=zookeeper:2181
      - SCHEMA_REGISTRY_HOST_NAME=schema-registry
      - SCHEMA_REGISTRY_LISTENERS=http://schema-registry:8081,http://localhost:8081
    ports:
      - "8081:8081"
    networks:
      - app-network
    depends_on:
      - zookeeper
      - kafka
    restart: unless-stopped

  # ================= Stroke Prediction API =================
  stroke-prediction-api:
    build: .
    container_name: stroke-prediction-api
    ports:
      - "8000:8000"
    volumes:
      - ./fast_api:/app/fast_api
      - ./models:/app/models
    working_dir: /app/fast_api
    environment:
      - PYTHONPATH=/app
      - PYTHONUNBUFFERED=1
      - MYSQL_HOST=${MYSQL_HOST}
      - MYSQL_PORT=${MYSQL_PORT}
      - MYSQL_DATABASE=${MYSQL_DATABASE}
      - MYSQL_USER=${MYSQL_USER}
      - MYSQL_PASSWORD=${MYSQL_PASSWORD}
    networks:
      - app-network
    depends_on:
      mysql:
        condition: service_healthy
    restart: on-failure
    command: >
      sh -c "sleep 15 && uvicorn main:app --host 0.0.0.0 --port 8000 --reload"
    healthcheck:
      test: ["CMD-SHELL", "python -c 'import urllib.request; urllib.request.urlopen(\"http://localhost:8000/model_info\")' || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 40s

  # ================= PostgreSQL for Analytics Cache =================
  postgres:
    image: postgres:13
    container_name: analytics-postgres
    environment:
      POSTGRES_DB: analytics_db
      POSTGRES_USER: analytics_user
      POSTGRES_PASSWORD: analytics_pass
    ports:
      - "5432:5432"
    networks:
      - app-network
    volumes:
      - postgres_data:/var/lib/postgresql/data
      - ./init/postgres-init.sql:/docker-entrypoint-initdb.d/init.sql
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U analytics_user -d analytics_db"]
      interval: 10s
      timeout: 5s
      retries: 5

  # ================= Spark Cluster + PySpark Notebook =================
  spark:
    image: thatojoe/spark:3.5.1
    hostname: spark
    container_name: spark-master
    environment:
      - SPARK_MODE=master
      - SPARK_RPC_AUTHENTICATION_ENABLED=no
      - SPARK_RPC_ENCRYPTION_ENABLED=no
      - SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no
      - SPARK_SSL_ENABLED=no
    ports:
      - "7077:7077"
      - "18080:8080"  # Changed from 8080 to avoid conflict
    networks:
      - app-network
      - spark-network
    restart: unless-stopped

  spark-worker-1:
    image: thatojoe/spark:3.5.1
    container_name: spark-worker-1
    environment:
      - SPARK_MODE=worker
      - SPARK_MASTER_URL=spark://spark:7077
      - SPARK_WORKER_MEMORY=1G
      - SPARK_WORKER_CORES=1
    networks:
      - app-network
      - spark-network
    depends_on:
      - spark
    restart: unless-stopped

  pyspark:
    image: jupyter/pyspark-notebook:latest
    container_name: pyspark
    hostname: pyspark
    ports:
      - "8888:8888"  # Changed from 9999 to standard Jupyter port
    volumes:
      - ./notebooks:/home/jovyan/work
      - ./jars:/home/jovyan/jars  # Volume for additional JDBC drivers
      - ./aws:/home/jovyan/.aws  # Volume for AWS credentials
    environment:
      - JUPYTER_ENABLE_LAB=no
      # All required JDBC drivers for PostgreSQL, MySQL, and MS SQL + S3 support
      - PYSPARK_SUBMIT_ARGS=--packages org.apache.spark:spark-avro_2.12:3.5.1,org.apache.hadoop:hadoop-aws:3.3.4,com.amazonaws:aws-java-sdk-bundle:1.12.262,org.postgresql:postgresql:42.6.0,mysql:mysql-connector-java:8.0.33,com.microsoft.sqlserver:mssql-jdbc:12.4.1.jre11 pyspark-shell
      # AWS S3 Configuration
      - AWS_ACCESS_KEY_ID=${AWS_ACCESS_KEY_ID}
      - AWS_SECRET_ACCESS_KEY=${AWS_SECRET_ACCESS_KEY}
      - AWS_DEFAULT_REGION=${AWS_DEFAULT_REGION:-us-east-1}
      - AWS_S3_BUCKET=${AWS_S3_BUCKET}
      - AWS_S3_ENDPOINT=${AWS_S3_ENDPOINT:-https://s3.amazonaws.com}
      # Spark S3 Configuration
      - SPARK_HADOOP_FS_S3A_ACCESS_KEY=${AWS_ACCESS_KEY_ID}
      - SPARK_HADOOP_FS_S3A_SECRET_KEY=${AWS_SECRET_ACCESS_KEY}
      - SPARK_HADOOP_FS_S3A_IMPL=org.apache.hadoop.fs.s3a.S3AFileSystem
      - SPARK_HADOOP_FS_S3A_AWS_CREDENTIALS_PROVIDER=org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider
      - SPARK_HADOOP_FS_S3A_CONNECTION_SSL_ENABLED=true
      # PostgreSQL connection environment variables
      - POSTGRES_HOST=postgres
      - POSTGRES_PORT=5432
      - POSTGRES_DB=analytics_db
      - POSTGRES_USER=analytics_user
      - POSTGRES_PASSWORD=analytics_pass
      # MySQL connection environment variables
      - MYSQL_HOST=mysql
      - MYSQL_PORT=3306
      - MYSQL_DATABASE=${MYSQL_DATABASE}
      - MYSQL_USER=${MYSQL_USER}
      - MYSQL_PASSWORD=${MYSQL_PASSWORD}
      # MS SQL connection environment variables
      - MSSQL_HOST=mssql
      - MSSQL_PORT=1433
      - MSSQL_DATABASE=master
      - MSSQL_USER=sa
      - MSSQL_PASSWORD=${MSSQL_SA_PASSWORD:-YourStrong!Pass123}
    networks:
      - app-network
      - spark-network
    depends_on:
      - spark
      - postgres
      - mysql
      - mssql
    restart: unless-stopped

  # ================= Airflow =================
  redis:
    image: redis:latest
    networks:
      - app-network
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 10s
      timeout: 5s
      retries: 5

  postgres_airflow:
    image: postgres:13
    container_name: postgres_airflow
    environment:
      POSTGRES_DB: airflow
      POSTGRES_USER: airflow
      POSTGRES_PASSWORD: airflow
    networks:
      - app-network
    volumes:
      - postgres_airflow_data:/var/lib/postgresql/data
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U airflow -d airflow"]
      interval: 10s
      timeout: 5s
      retries: 5

  # Airflow initialization
  airflow-init:
    image: apache/airflow:2.7.1
    container_name: airflow-init
    environment:
      AIRFLOW__CORE__EXECUTOR: CeleryExecutor
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@postgres_airflow/airflow
      AIRFLOW__CELERY__RESULT_BACKEND: db+postgresql://airflow:airflow@postgres_airflow/airflow
      AIRFLOW__CELERY__BROKER_URL: redis://redis:6379/0
      AIRFLOW__CORE__LOAD_EXAMPLES: 'false'
      AIRFLOW__CORE__LOAD_DEFAULT_CONNECTIONS: 'false'
      AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION: 'false'
      # AWS S3 for Airflow
      AWS_ACCESS_KEY_ID: ${AWS_ACCESS_KEY_ID}
      AWS_SECRET_ACCESS_KEY: ${AWS_SECRET_ACCESS_KEY}
      AWS_DEFAULT_REGION: ${AWS_DEFAULT_REGION:-us-east-1}
    volumes:
      - ./dags:/opt/airflow/dags
      - ./scripts:/opt/airflow/scripts
      - ./logs:/opt/airflow/logs
    networks:
      - app-network
    depends_on:
      postgres_airflow:
        condition: service_healthy
      redis:
        condition: service_healthy
    user: "0:0"
    command: >
      bash -c "
        mkdir -p /opt/airflow/logs/scheduler /opt/airflow/logs/worker /opt/airflow/logs/webserver &&
        chown -R 50000:0 /opt/airflow/logs &&
        chmod -R 755 /opt/airflow/logs &&
        chmod -R 755 /opt/airflow/dags &&
        echo 'Waiting for database to be ready...' &&
        until pg_isready -h postgres_airflow -p 5432 -U airflow; do
          echo 'Waiting for database...';
          sleep 5;
        done &&
        echo 'Initializing Airflow database...' &&
        airflow db init &&
        echo 'Creating admin user...' &&
        airflow users create --username admin --password admin --firstname Admin --lastname User --role Admin --email admin@example.com || true &&
        echo 'Airflow initialization complete'
      "

  airflow-webserver:
    image: apache/airflow:2.7.1
    container_name: airflow-webserver
    environment:
      AIRFLOW__CORE__EXECUTOR: CeleryExecutor
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@postgres_airflow/airflow
      AIRFLOW__CELERY__RESULT_BACKEND: db+postgresql://airflow:airflow@postgres_airflow/airflow
      AIRFLOW__CELERY__BROKER_URL: redis://redis:6379/0
      AIRFLOW__CORE__LOAD_EXAMPLES: 'false'
      AIRFLOW__CORE__LOAD_DEFAULT_CONNECTIONS: 'false'
      AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION: 'false'
      # AWS S3 for Airflow
      AWS_ACCESS_KEY_ID: ${AWS_ACCESS_KEY_ID}
      AWS_SECRET_ACCESS_KEY: ${AWS_SECRET_ACCESS_KEY}
      AWS_DEFAULT_REGION: ${AWS_DEFAULT_REGION:-us-east-1}
    volumes:
      - ./dags:/opt/airflow/dags
      - ./scripts:/opt/airflow/scripts
      - ./logs:/opt/airflow/logs
    ports:
      - "8080:8080"
    networks:
      - app-network
    depends_on:
      - airflow-init
    command: webserver
    healthcheck:
      test: ["CMD", "curl", "--fail", "http://localhost:8080/health"]
      interval: 30s
      timeout: 10s
      retries: 5
    restart: unless-stopped

  airflow-scheduler:
    image: apache/airflow:2.7.1
    container_name: airflow-scheduler
    environment:
      AIRFLOW__CORE__EXECUTOR: CeleryExecutor
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@postgres_airflow/airflow
      AIRFLOW__CELERY__RESULT_BACKEND: db+postgresql://airflow:airflow@postgres_airflow/airflow
      AIRFLOW__CELERY__BROKER_URL: redis://redis:6379/0
      AIRFLOW__CORE__LOAD_EXAMPLES: 'false'
      AIRFLOW__CORE__LOAD_DEFAULT_CONNECTIONS: 'false'
      AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION: 'false'
      # AWS S3 for Airflow
      AWS_ACCESS_KEY_ID: ${AWS_ACCESS_KEY_ID}
      AWS_SECRET_ACCESS_KEY: ${AWS_SECRET_ACCESS_KEY}
      AWS_DEFAULT_REGION: ${AWS_DEFAULT_REGION:-us-east-1}
    volumes:
      - ./dags:/opt/airflow/dags
      - ./scripts:/opt/airflow/scripts
      - ./logs:/opt/airflow/logs
    networks:
      - app-network
    depends_on:
      - airflow-init
    command: scheduler
    healthcheck:
      test: ["CMD-SHELL", "airflow jobs check --job-type SchedulerJob --hostname airflow-scheduler"]
      interval: 30s
      timeout: 10s
      retries: 5
    restart: unless-stopped

  airflow-worker:
    image: apache/airflow:2.7.1
    container_name: airflow-worker
    environment:
      AIRFLOW__CORE__EXECUTOR: CeleryExecutor
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@postgres_airflow/airflow
      AIRFLOW__CELERY__RESULT_BACKEND: db+postgresql://airflow:airflow@postgres_airflow/airflow
      AIRFLOW__CELERY__BROKER_URL: redis://redis:6379/0
      AIRFLOW__CORE__LOAD_EXAMPLES: 'false'
      AIRFLOW__CORE__LOAD_DEFAULT_CONNECTIONS: 'false'
      AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION: 'false'
      # AWS S3 for Airflow
      AWS_ACCESS_KEY_ID: ${AWS_ACCESS_KEY_ID}
      AWS_SECRET_ACCESS_KEY: ${AWS_SECRET_ACCESS_KEY}
      AWS_DEFAULT_REGION: ${AWS_DEFAULT_REGION:-us-east-1}
    volumes:
      - ./dags:/opt/airflow/dags
      - ./scripts:/opt/airflow/scripts
      - ./logs:/opt/airflow/logs
    networks:
      - app-network
    depends_on:
      - airflow-init
    command: celery worker
    healthcheck:
      test: ["CMD-SHELL", "timeout 30s airflow celery inspect ping -d celery@$$HOSTNAME"]
      interval: 60s
      timeout: 40s
      start_period: 120s
      retries: 3
    restart: unless-stopped

  # ================= Superset =================
  superset:
    image: apache/superset:latest
    container_name: superset
    environment:
      - SUPERSET_SECRET_KEY=your-super-secret-key-change-this
      - DATABASE_DB=superset
      - DATABASE_USER=superset
      - DATABASE_PASSWORD=superset
      - DATABASE_HOST=postgres_superset
      - DATABASE_PORT=5432
    ports:
      - "8088:8088"
    networks:
      - app-network
    depends_on:
      - postgres_superset
    volumes:
      - superset_data:/app/superset_home
    command: >
      bash -c "
        sleep 30 &&
        superset db upgrade &&
        superset init &&
        superset run -p 8088 --host=0.0.0.0
      "

  postgres_superset:
    image: postgres:13
    container_name: postgres_superset
    environment:
      - POSTGRES_DB=superset
      - POSTGRES_USER=superset
      - POSTGRES_PASSWORD=superset
    networks:
      - app-network
    volumes:
      - postgres_superset_data:/var/lib/postgresql/data
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U superset -d superset"]
      interval: 10s
      timeout: 5s
      retries: 5

# ================= Networks =================
networks:
  app-network:
    driver: bridge
  spark-network:
    driver: bridge

# ================= Volumes =================
volumes:
  postgres_data:
  postgres_airflow_data:
  postgres_superset_data:
  superset_data:
  mssql_data:
